% document and page typologies
\documentclass[a4paper, onecolumn]{report}

% packages loading
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{mathptmx}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage[margin=1pt, font=small]{caption}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsbsy}
\usepackage{bm}
\usepackage{morefloats}
\usepackage{lettrine}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{listing}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


% text style
\geometry{top=2cm, left=2cm, right=2cm, bottom=3cm}
%\renewcommand*\thesubsection{\alph{subsection}}
\titlespacing{\title}{0pt}{*5}{*0}
\titlespacing{\section}{0pt}{*2}{*0}
\titlespacing{\subsection}{0pt}{*1}{*0}
\titlespacing{\subsubsection}{0pt}{*1}{*0}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{b}{n}
\newcommand{\newln}{\right.  \notag \\ &\left.}
\setlength{\intextsep}{10pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{1pt} % Vertical space below (above) [t] ([b]) floats
\setlength{\abovecaptionskip}{10pt}
\setlength{\belowcaptionskip}{10pt}
\font\largefont=yinitas
%\renewcommand{\labelitemii}{$\bullet$}

\begin{document}

\chapter{Related work}
\section{Introduction}
Lorem ipsum dolor sit amet, consectetur adipisci elit, sed eiusmod tempor incidunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur. Quis aute iure reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint obcaecat cupiditat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\section{Related work in scan matching}
Given two sets of 2D data, the \emph{scan matching} problem consists in finding a traslation $T$ and a rotation $R_\phi$ that maximize the overlapping between the two sets. The scan matching algorithms usually work using data furnished by 2D range sensors and differ in their behaviour depending on the availability of an initial guess of solution. \\
A further classification of scan matching algorithms can be made on the basis of their assumption about the presence of noise in sensor's measurements and of features in surrounding environment. Feature-based algorithms, in particular, gained a great success because of their computational effectiveness, but it must be underlined that the extraction of features may produce a loss of information. 

When ambient contains features that are invariant to rotation and traslation, they can be easily extracted from scans and can be used to find a solution in a linear time. \\
If the extraction of features is not simple because of the structure of environment, then it is possible to use algorithms belonging to ICP family, which base their operation on a two step procedure:
\begin{itemize}
	\item{find an initial heuristic set of correspondences between points in two scans}
	\item{find a roto-traslation that more or less satisfies the set of correspondences}
\end{itemize}
The two steps are repeated until the error falls below a certain threshold. The convergence is possible if scan are produced in two robot's positions that are close to each other.

\subsection{Scan matching using the Hough transform}
In a 2009 article by Censi, Iocchi and Grisetti, the problem of scam matching is afforded by the use of \emph{Hough transform (HT)} and is, then, moved to the Hough domain. If we have an \emph{input space} $\mathcal{S}$, the HT maps an input $i(\bf{s})$ (with $\bf{s} \in \mathcal{S}$) to an output function $\mbox{HT}\{i\}(\bf{p})$, with $\bf{p}$ belonging to a \emph{parameter space} $\mathcal{P}$. \\
We can define the HT of input $i(\bf{s})$ as
\begin{equation}
	\mbox{HT}[\mathcal{F}, i](\bf{p}) = \int_{\mathcal{F}_{\bf{p}}} \emph{i}(\bf{s}) \emph{d}\bf{s}
\end{equation}
A possible choice for $\mathcal{P}$ is the the set of representations of lines in $\mathbb{R}^2$. If we recall the polar representation of lines 
\begin{equation}
	x\cos\theta + y\sin\theta = \rho
\end{equation}
in which $\theta$ expresses the direction of line's normal vectors and $\rho$ its distance from origin, we can select a family of sets to apply HT:
\begin{equation}
	\mathcal{F}_{(\theta, \rho)} =\{(x,y) | x\cos\theta + \sin\theta = \rho\}
\end{equation}
On the other side, the input space is composed by points $P = \{p_j\}$, as they are returned by a range sensor. So we have that
\begin{equation}
	\bf{s} := \emph{(x,y)} \in \mathcal{S} := \mathbb{R}^\emph{2}
\end{equation}
and
\begin{equation}
	i(\bf{s}) = \sum_{\emph{j}} \delta(\bf{s} - \bf{p_j})
\end{equation}
where $\delta$ is Dirac's impulse distribution.

HT has two fundamental properties:
\begin{itemize}
	\item{\mbox{HT}$(\theta, \rho)$ is 2$\pi$-periodic}
	\item{the two points $(\theta, \rho)$ and $(\theta+\pi, -\rho)$ in $\mathcal{P}$ represent the same line in $\mathbb{R}^2$}
\end{itemize}
Let's suppose that $\emph{i}(\bf{s})$ and $\emph{i'}(\bf{s})$ are two inputs, such that
\begin{equation}
	\emph{i'}(\bf{s}) = \emph{i}(\emph{R}_\phi \cdot \bf{s} + \emph{T})
\end{equation}
Then, within parameter space $\mathcal{P}$:
\begin{equation}
	\mbox{HT}'(\rho, \theta) = \mbox{HT}(\theta+\phi, \rho+(\cos\theta \quad \sin\theta)T)
\end{equation}
Computing the HT passes through a discrete approximation of it, called \emph{Discrete Hough Transform (DHT)}, that allows to redefine our family of sets as:
\begin{equation}
	\mathcal{D}_{(\theta, \rho)} = \{(x,y) | \ \rho\le x\cos\theta + y\sin\theta < \rho + \Delta\rho\} 
\end{equation}
%--------------------------------------------------------------------------
\chapter{Basic concepts}
\section{Introduction}
This chapter introduces to the main concepts on which this thesis is based. 
First of all, we will concentrate ourselves on describing the concept of sensors and their importance in self-perception of robots. Sensors, indeed, are very useful to allow a robot to localize itself and have an idea of surrounding environment; anyway, to make all this possible, it is necessary to design them in such a way to satisfy a series of requirements. Sensors, although their immense utility, furnish noisy measurements with which localization algorithms must deal.
In this chapter, we will describe one particular kind of sensors, the ones based on the \emph{time of flight (TOF)} method.

The main tecnique used by robots to estimate their current position and orientation is \emph{odometry}, which is based on data returned by robot's encoders. We will analyse the two types of errors it introduces and how they have been experimentally measured.

Data furnished by sensors is often huge, so it is important to extract only relevant parts of it, in order to reduce time in computation. We will describe how such parts can be extracted and which properties we expect them to have.

Finally, we will propose a process based on least squares minimization, that allows to estimate a robot's state (in terms of position and orientation), exploiting the error between measured and expected state value.

\section{Robot sensors}
Robots can be considered, among human artefacts, as the ones closest to the concept of a living being, with their capabilities to sense world and change their own choices on the basis of what they perceive in the surrounding environment. Without a sensing system, robots could be just able to repeat the same task and wouldn't be able to adapt their decisions to an environment continously changing around them. Thanks to sensors, robots can operate in places that are too dangerous for humans and interact with each other. 

When designing a sensor, there is a small set of requirements that should be kept into account:
\begin{itemize}
\item{a good field of view, to satisfy the needings of the greatest number of applications}
\item{a good minimum and maximum range of detection}
\item{the largest accuracy and resolution}
\item{a great ability to detect objects in the environment, dealing with phenomena such as reflection and noise due to ambient interference}
\item{high update frequency, to have real time data at disposal as fast as possible}
\item{generated data should be concise and easy to interpret}
\item{low costs in designing, construction and maintenance}
\item{low power consumption}
\item{small size and weight}
\end{itemize}

\subsection{Range sensors}
In many cases, a robot is required to build a map of its surrounding environment and for such goal a measurement of range between itself and obstacles is needed.
There exists a class of range sensors widely used for such purpose based on the so called \emph{time of flight (TOF)} method; an ultrasonic, RF or optical source generates an energy wave that propagates into ambient and the distance between robot and an object is computed as the product of the velocity of wave and the time required to travel the round-trip distance, according to relation

\begin{equation}
	d = vt
\end{equation}
where $d$ is the round-trip distance, $v$ is the speed of propagation of energy wave and $t$ is the elapsed time. The time interval $t$ in previous equations is the one that elapses while the energy wave reaches the obstacle and gets back to robot; thus, to retrieve the real range between robot and target, $t$ must be divided by two.

The signal emitted by robot reaches an object and walks back on the same way to the robot, where it will be detected by a receiver, that can be located close to emitter or integrated with emitter itself.
As it is evident in ( ), the range to an object can be easily retrieved and no further knowledge about object's nature is necessary.

Similarly to all kinds of sensors, even TOF systems are subject to measurements errors, which can be summarized in the following short list:
\begin{itemize}
\item{\bf{variations in the speed of propagation of emitted signal}}
\item{\textbf{uncertainty in determining when reflected signal was detected}\\ 
Such uncertainties depend on the fact that different surfaces reflect signal differently and this may cause time for detection of returned signal to rise. When a threshold on detection is established, more reflective objects are perceived as closer.}
\item{\textbf{uncertanty in determining the exact round-trip time $t$}\\
 It is due to problems correlated to inner timing circuitry.}
\item{\textbf{interaction between originated signal and surfaces} \\
When source signal hits a surface, only part of it is reflected and perceived by the robot. The remaining part is propagated into ambient or passes through the hit object. Thus, it may happen that no signal is received by robot, particularly if emitting angle exceeds a certain threshold. The part of signal that is not sent back to robot, may be spread to envirnonment and perceived by another robot's sensors.}
\end{itemize}

\section{Odometry}
Odometry is probably the most used method to estimate the position of a robot at any instant in time. The reasons of its success must be found in good accuracy, low cost and high sampling rates. Anyway, the way odometry computes robot's position (by integrating motion information across time) leads to errors; in particular, errors in estimating robot's orientation lead to large position errors, which grow proportionally with the distance afforded by robot. \\
Despite limitations due to errors, odometry gained a great success in mobile robots field, for at the least four reasons:
\begin{itemize}
\item{other kinds of measurements can be itegrated to obtain a better accuracy}
\item{in cases in whick landmarks are used for helping to estimate robot's position, their number can be smaller, thanks to odometry's presence}
\item{odometry allows to assume that robot is stuck in a certain pose for enough time to perceive all the landmarks in a given area and compute possible matchings with landmarks detected previously}
\item{in some circumstances, because of the absence of external references (e.g. landmarks) or the impossibility to place any of them in a hostile environment, odometry can be the only way to estimate robot's position}
\end{itemize}
Odometry exploits the measurements obtained from encoders mounted on robot's wheels to extimate its new pose. Let's suppose that, in a certain time interval, robot's wheels have pulses $N_L$ and $N_R$. Let's denote with $c_m$ the conversion factor that allows to convert pulses into a linear displacement. We can suppose that
\begin{equation}
	c_m = \frac{\pi D_n}{nC_e}
\end{equation}
where:
\begin{itemize}
\item{$D_m$ is the nominal wheel's diameter}
\item{$C_e$ is the encoder resolution (pulses per revolution)}
\item{$n$ is the reduction gear ratio between motor and drive wheel}
\end{itemize}
The incremental linear displacements $\Delta U_{L,i}$ and $\Delta U_{R,i}$ for the two wheels can be computed as:
\begin{equation}
	\begin{aligned}
		\Delta U_{L,i} &= c_m N_L \\
		\Delta U_{R,i} &= c_m N_R
	\end{aligned}
\end{equation}

We can now compute the displacement for robot's centerpoint $\Delta U_i$ (that will be taken as a reference for robot's position) and change in orientation $\Delta \theta_i$:
\begin{equation}
	\begin{aligned}
		\Delta U_i &= \frac{\Delta U_R + \Delta U_L}{2}  \\
		\Delta \theta_i &= \frac{\Delta U_R - \Delta U_L}{b}
	\end{aligned}				
\end{equation}
where $b$ is the distance between the two points where wheels touch the ground. 

Thus, the new orientation of robot at time $i$ is:
\begin{equation}
	\theta_i = \theta_{i-1} + \Delta \theta_i
\end{equation}
The new position, at the same instant of time, of its centerpoint will be:
\begin{equation}
	\begin{aligned}
		x_i &= x_{i-1} + \Delta U_i \cos \theta_i \\
		y_i &= y_{i-1} + \Delta U_i \sin \theta_i
	\end{aligned}
\end{equation}

\subsection{Systematic and Non-Systematic errors in odometry}
As explained previously, the odometry exploits three simple equations to estimate current position of robot. All of them, are based on the assumption that any encoder's measurement can be translated into a linear diplacement. Anyway, such assumption may lose its validity in certain circumstances; for instance, if a wheel slips, its encoder will register a measurement that is converted in a linear displacement, even if robot didn't move actually. \\
Slippage is only one of the possible causes of errors in odometry; they can me summarized and divided into the following two categories:\\ \\
\textbf{Systematic errors}
\begin{itemize}
	\item{wheels with different diameters}
	\item{wheelbase different from nominal one}
	\item{wheels misalignment}
	\item{finite encoder resolution}
	\item{finite encoder sampling rate}
\end{itemize} 
\textbf{Non-Systematic errors}
\begin{itemize}
	\item{travelling on uneven floors}
	\item{travelling over objects}
	\item{slippage}
\end{itemize}

Both errors categories are very difficult to manage; the first ones, in fact, are constantly present and accumulate over time; the latter, are unpredictable and robot must be able to react promptly when they manifest.

Across time, algorithms managing robot's position have thought it as surrounded by an error ellipse, denoting an area where robot may be in a certain moment. Ellipses grow with distance travelled, until an absolute measurement of position reduces ellipse's size. This method based on ellipses can be used to contain systematic errors only, since non systematic ones are unpredictable.

Estimating correctly the extent of odometric errors avoids further problems such as a wrong calibration of mobile platforms. In 1995, Borenstein and Feng developed a method based on a model which considers two erros to be dominant:
\begin{itemize}
	\item{the error due to different wheels' diameters $E_d = \frac{D_R}{D_L}$, with $D_R$ and $D_L$, respectively, the actual diameters of right and left wheel}
	\item{the error due to uncertainty about real wheelbase $E_b = \frac{b_{actual}}{b_{nominal}}$, with $b$ the wheelbase of robot}
\end{itemize}

\subsection{Measuring Systematic errors}
Before describing Borenstein and Feng's method to estimate systematic errors, it is useful to analyse Borenstein and Koren's method (1987). \\
Let's suppose that robot starts its path from an initial position <$x_0, y_0, \theta_0$> and has to move on a 4x4 meter square path. The robot is programmed to return to its initial position at the end of the path, but because of systematic errors, this will not happen accurately and a series of errors will be accumulated (\emph{return position errors}):
\begin{equation}
	\begin{aligned}
		\epsilon_{x} &= x_{abs}-x_{calc} \\
		\epsilon_{y} &= y_{abs}-y_{calc} \\
		\epsilon_{\theta} &= \theta_{abs}-\theta_{calc}
	\end{aligned}
\end{equation}

We have denoted with the $abs$ subscript the absolute position and orientation of the robot and with $calc$ subscript the position and orientation of robot according to odometry.

This kind of experiment is not suitable for testing odometry for differential drive platforms. That is why Borenstein and Feng introduced their \emph{bidirectional square-path} experiment. This time, the robot is required to walk the path both in clockwise and counterclockwise direction. The errors $E_d$ and $E_b$ could compensate each other when robot travelled on one direction only; now, travelling on both direction, the two errors sum up.

If we let the robot travel on both directions $n$ times (usually, $n=5$), at the end of each run robot will accumulate a pose error as explained in (). We can compute the center of gravity of these errors according to the following relations:
\begin{equation}
	\begin{aligned}
		x_{cg}^{cw} &= \frac{1}{n}\sum^n_{i=1}\epsilon_{x,i}^{cw} \\
		y_{cg}^{cw} &= \frac{1}{n}\sum^n_{i=1}\epsilon_{y,i}^{cw} \\
		x_{cg}^{ccw} &= \frac{1}{n}\sum^n_{i=1}\epsilon_{x,i}^{ccw} \\
		y_{cg}^{ccw} &= \frac{1}{n}\sum^n_{i=1}\epsilon_{y,i}^{ccw} 
	\end{aligned}
\end{equation}
The two absolute offsets of centers of gravity from origin are:
\begin{equation}
	\begin{aligned}
		r_{cg}^{cw} &= \sqrt{(x_{cg}^{cw})^2+(y_{cg}^{cw})^2} \\
		r_{cg}^{cw} &= \sqrt{(x_{cg}^{ccw})^2+(y_{cg}^{ccw})^2}
	\end{aligned}
\end{equation}
Finally, a measure of systematic odometric error can be obtained as
\begin{equation}
	E_{sys} = \max(r_{cg}^{cw},r_{cg}^{ccw}).
\end{equation}
The orientation error $\epsilon_\theta$ is not considered in $E_{sys}$, since each orientation error translates into a position error.

\subsection{Measuring non-Systematic errors}
The square path test that was used to estimate systematic error, can be implemented again for non-systematic case, adding to the path some artificial bumps. Since return errors are sensible to the positions of bumps, this time error $\epsilon_\theta$ will be used. We can, then, compute an \emph{average absolute orientation error}:
\begin{equation}
	\epsilon_{\theta, avg}^{nonsys} = \frac{1}{n}\sum_{i=1}^{n}|\epsilon_{\theta,i, cw}^{nonsys}-\epsilon_{\theta,i, cw}^{sys}| + \frac{1}{n}\sum_{i=1}^{n}|\epsilon_{\theta,i, ccw}^{nonsys}-\epsilon_{\theta,i, ccw}^{sys}|
\end{equation}
The use of absolute values in previous equations is needed to avoid that two return orientation errors with opposite signs compensate each other. Thus, if after first run we have $\epsilon_\theta=1^\circ$ and after second one we have $\epsilon_\theta = -1^\circ$, we will not erroneously derive that $\epsilon_{avg}^{nonsys}=0^\circ$.

\section{Extracting features}
In many different fields (machine learning and pattern recognition, just to mention a pair) the amount of data to manage (for example, the measurements returned by robot sensor) is too huge to be used in toto, so it becomes of great importance to extract only the necessary information, removing, moreover, all the redundancies that could make computation unfeasible. Thus., it is convenient to design processes that are able to extract information that is representative of data and allows to overcome the problem of using data in its totality. Such processes perform what is called \emph{features extraction}, i.e. they retrieve (and sometimes compute) elements (\emph{features}) that are distinctive of data, with which it is easier to operate; features are required to be informative, discriminative and indipendent. For some uses, features are even required to mantain some properties across space and time. \\
Usually, features have a numerical nature, but sometimes they can present a different aspect (in some cases they can be strings of characters or histograms).

Before designing an extraction process, features' structure needs to be planned, in order to respect the goal of good representation of data and to obtain elements with which working will be easy and effective.
The choice about structure and computation of features depends on their final use and algorithms' purposes; here is a short list of common choices made in different fields:
\begin{itemize}
	\item{histograms of the distribution of black pixels in characters recognition}
	\item{phonemes in speech recognition}
	\item{repetitive words or text structures in spam detection, inside email inboxes}
	\item{edges and corners in computer vision}
\end{itemize}

As explained in previous section, data retrieved from odometry are often inaccurate and they can lead to errors in robot's position estimation; features, after their extraction from sensors' data, can add a precious information allowing to reduce the errors and gain a better estimate of robot position in time. Indeed, each feature carries information about its reciprocal position with respect to robot and it can be used to improve localization methods\\
It is necessary to underline that even a well designed process of extraction can return a number of features that is too huge to be handled; so, it becomes crucial, in such cases, to be aware about which features are really necessary and which can be ignored. This mainly depends on the context in which robot operates, the perception system and the way features are represented.

\subsection{Extracting lines as features}
When receiving data from a ranger sensor (for instance, a laser range), one needs to operate with pairs $(\rho_i, \theta_i)$, which denote the position of i-th point in space, in polar coordinates, in robot's reference frame. Retrieving features from such information can be performed in two different ways:
\begin{itemize}
	\item{use a subset of points as features}
	\item{extract lines from the sequence of points, representing the part of environment seen by sensor during last scan}
\end{itemize}
For the purpose of this thesis, we will consider the second option.

Across the years, many algorithms have been developed for extracting lines, starting from a laser's sequence of points, each with its pros and cons. Here, we will describe the so-called \emph{split and merge algorithm}, which operates using a divide et impera logic, starting with an initial set of points, progressively divided into subsets (on the basis of a check condition), until satisfying the conditions for determining the existence of a line (the subset under inspection has not enough points to be divided into two subsets and/or no point satisfies check condition to proceed to another division).
The main steps of procedure are described by Algorithm 1.
\begin{algorithm}
\caption{Split and merge algorithm}\label{euclid}
\begin{algorithmic}[1]
\item{Initial: put all points in a set $s_1$ and insert $s_1$ into an initially empty list $\mathcal{L}$}
\item{Get a set $s_i$ from $\mathcal{L}$ and fit a line through its points}
\item{Find point $P$ having the greatest distance $d_P$ from line}
\item{If $d_P$ is less than a threshold, go back to step 2}
\item{Otherwise, split $s_i$ in $P$ into two subsets $s_{i1}$ and $s_{i2}$. Replace $s_i$ with $s_{i1}$ and $s_{i2}$ in $\mathcal{L}$. Go back to step 2}
\item{If all sets in $\mathcal{L}$ have been checked, merge collinear lines}
\end{algorithmic}
\end{algorithm}
The second step of algorithm can be performed using any fitting process, even by connecting the first and last point in the sequence $s_i$. Each computed line is represented by its two extremes.

\section{Least squares estimation}
Let \textbf{x} be a vector variable indicating the state of the system and let's suppose to have a series of  functions 
\begin{equation}
	\{f_i(\textbf{x})\}_{i = 1:n} \notag
\end{equation}
which, given \textbf{x}, predict its measurement. \\Let's suppose, moreover, to have a series of real measurements ${\bf z_i}$. The goal is to extimate the state ${\bf x}$ that best fits the measurements ${\bf z_{i:n}}$.\\
We can, then, determine an error between each real measurement and the relative predicted one:
\begin{equation}
	\textbf{e}_i = \textbf{z}_i - f_i(\textbf{x})
\end{equation}
This error is assumed to be zero mean and normally distributed with information matrix \textbf{$\Omega_i$}. \\
If we compute the squared form of error, we notice that it is a scalar and depends only on the state of the system:
\begin{equation}
	e_i(\textbf{x}) = \textbf{e}_i(\textbf{x})^T\Omega_i\textbf{e}_i(\textbf{x})
\end{equation}
Then, the problem of finding the state that best fits the real measurements can be reduced to find the state ${\bf x^*}$ such that
\begin{align}
	\textbf{x*} &= \underset{\textbf{x}}{\mbox{argmin}} \enspace F(\textbf{x}) \\ \notag
	&= \underset{\textbf{x}}{\mbox{argmin}} \sum_i e_i(\textbf{x}) \\ 
	&= \underset{\textbf{x}}{\mbox{argmin}} \sum_i  \textbf{e}_i^T(\textbf{x})\Omega_i\textbf{e}_i(\textbf{x}) \notag
\end{align}
A general solution to the problem is to derive the global error function and find its nulls, but this would result in a complex solution which doesn't admit closed forms. So, proceeding with numerical approaches is a more practical path. 

If a good initial guess is at disposal and the measurement functions are smooth in the neighborhood of minima, we can solve problem using iterative (local) linearizations. The iterative process is composed of three steps:
\begin{itemize}
	\item{linearize the problem around current initial guess}
	\item{solve a linear system}
	\item{compute a set of increments to be applied to current estimate in order to get closer to minima}
\end{itemize}
The first step is linearizing the problem and this can be done using a Taylor expansion:
\begin{align}
	\textbf{e}_i(\textbf{x} + \Delta\textbf{x}) &\simeq \textbf{e}_i(\textbf{x}) + \frac{\partial \textbf{e}_i}{\partial \textbf{x}}\Delta\textbf{x}\\
	&= \textbf{e}_i + \textbf{J}_i\Delta\textbf{x} \notag
\end{align}
We can now place the Taylor expansion in the squared error:
\begin{align}
	&e_i(\textbf{x} + \Delta\textbf{x}) \simeq \notag \\
	&\textbf{e}_i^T\Omega_i\textbf{e}_i + \textbf{e}_i^T\Omega_i\textbf{J}_i\Delta\textbf{x} + \Delta\textbf{x}^T\textbf{J}_i^T\Omega_i\textbf{e}_i + \Delta\textbf{x}^T\textbf{J}^T_i\Omega_i\textbf{J}_i\Delta\textbf{x} \notag \\
	&= \underbrace{\textbf{e}_i\Omega_i\textbf{e}_i}_\text{$c_i$} + 2\underbrace{\textbf{e}_i^T\Omega_i\textbf{J}_i}_\text{$b_i^T$}\Delta\textbf{x} + \Delta\textbf{x}^T\underbrace{\textbf{J}_i^T\Omega_i\textbf{J}_i}_\text{$\textbf{H}_i$}\Delta\textbf{x} \notag \\
	&= c_i + 2\textbf{b}_i^T\Delta\textbf{x} + \Delta\textbf{x}^T\textbf{H}_i\Delta\textbf{x}
\end{align}
The global error is the sum of the squared errors, so, from (3) and (5), we have:
\begin{align}
	F(\textbf{x} + \Delta\textbf{x}) &\simeq \sum_i(c_i + 2\textbf{b}_i^T\Delta\textbf{x} + \Delta\textbf{x}^T\textbf{H}_i\Delta\textbf{x}) \notag \\
	&= \sum_i c_i + 2(\sum_i\textbf{b}_i^T)\Delta\textbf{x} + \Delta\textbf{x}^T(\sum_i\textbf{H}_i)\Delta\textbf{x}
\end{align}
where
\begin{align}
	\textbf{b}^T &= \sum_i\textbf{e}_i^T\Omega_i\textbf{J}_i  \\
	\textbf{H} &= \sum_i\textbf{J}_i^T\Omega_i\textbf{J}_i 
\end{align}
The expression of global error we have just obtained is quadratic in $\Delta\textbf{x}$ and can be minimized; for this purpose, we can compute its derivative with repect to $\Delta\textbf{x}$ in the neighborhood of current solution $\textbf{x}$:
\begin{equation}
\frac{\partial F(\textbf{x}+\Delta\textbf{x})}{\partial\Delta\textbf{x}} \simeq 2\textbf{b} + 2\textbf{H}\Delta\textbf{x}
\end{equation}
with the optimal solution reached for 
\begin{equation}
\Delta\textbf{x*} = -\textbf{H}^{-1}\textbf{b}
\end{equation}
Thus, an iterative algorithm can be designed; it will execute the following steps:
\begin{itemize}
	\item{linearize the system around current guess $\textbf{x}$ and compute $\textbf{e}_i(\textbf{x} + \Delta\textbf{x})$ as described in () and ()}
	\item{compute $\textbf{b}^T$ and $\textbf{H}$ as described in () and ()}
	\item{solve the system to get a new optimal increment $\Delta\textbf{x*}$, as indicated in ()}
	\item{update the previous estimate: $\textbf{x} \leftarrow \textbf{x} + \Delta\textbf{x*}$}
\end{itemize}

\end{document}